from __future__ import annotations

import operator
import time
from dataclasses import dataclass, field
from functools import partial, reduce
from typing import Callable, Any, Tuple, List

import jax
import numpy as np
from absl import logging, flags
from flax.jax_utils import replicate, unreplicate
from flax.training.train_state import TrainState
from jaxtyping import Num, Array
from rich.console import Console
from rich.layout import Layout
from rich.live import Live
from tensorflow import data as tfd  # Only for typing

from chemise.callbacks.abc_callback import Callback, CallbackRunner, CallbackFn
from chemise.traning.prefetch import Prefetch
from chemise.utils import mean_reduce_dicts, make_metric_string, seconds_pretty

flags.DEFINE_bool("interactive", default=False, help="Run in interactive mode. e.g print graphs", short_name='i')
flags.DEFINE_float("refresh_per_second", default=0.2, help="Frequency in Hz to redraw in interactive mode")

FLAGS = flags.FLAGS

State_Result = Tuple[TrainState, dict]
Batch = Tuple[dict[str, Num[Array, "..."]], dict[str, Num[Array, "..."]]]


def empty_train_hist():
    return {"epochs": [], "train": []}


def make_default_layout() -> Layout:
    layout = Layout(name="root")
    layout.split(
        Layout(name="graph", ratio=1),
        Layout(name="progbar", size=5),
    )
    return layout


def add_device_batch(data: tfd.Dataset) -> tfd.Dataset:
    platform = jax.default_backend()
    d_count = jax.device_count(platform)
    logging.log_first_n(logging.INFO, "Running on %s with %d devices", 1, platform, d_count)
    logging.debug("Adding device batch of size (%d) to dataset: %s", d_count, data.element_spec)
    data = data.batch(d_count, drop_remainder=True).prefetch(2)
    return data

def sanity_check(data: tuple[dict[str, Num[Array, "..."]], dict[str, Num[Array, "..."]]]):
    """
    Check to see if the input and label data looks correct, i.e not all the same value
    :param data:
    :return: bool - True if all values different, dict - input keys and a bool set to True if value is all the same
    """
    inputs = {f"I_{k}": np.all(v == v[0], axis=None) for k, v in data[0].items()}
    r_inputs = reduce(operator.or_, inputs.values(), False)
    labels = {f"O_{k}": np.all(v == v[0], axis=None) for k, v in data[1].items()}
    r_labels = reduce(operator.or_, labels.values(), False)
    is_good = np.logical_not(np.logical_or(r_inputs, r_labels))
    return is_good, dict(**inputs, **labels)

@jax.tree_util.Partial
def no_metrics_fn(y, y_hat):
    return {}

@dataclass(unsafe_hash=True)
class BasicTrainer:
    """
    Implement boilerplate helper methods to fit basic models similar to what we get in keras
    This class can manage all the state for the various jax / flax object needed to run basic NN training
    We want to have a few basic methods
     - Fit
     - Predict
     - TODO: Transform - same as predict but just add the predictions the input data
    """

    state: TrainState = field(compare=False)
    loss_fn: Callable[[Num[Array, "..."] | dict[str, Num[Array, "..."]], Num[Array, "..."]], Num[Array, "..."]]
    metrics_fn: Callable[[Num[Array, "..."], Num[Array, "..."]], dict] = no_metrics_fn
    callbacks: [Callback] = field(default_factory=list, compare=False)
    train_hist: dict[str, list[Any]] = field(default_factory=empty_train_hist, compare=False)
    train_window: Layout = field(default_factory=make_default_layout, compare=False)

    rng_keys: List[str] = field(default_factory=list, compare=False)
    seed: int = 0
    _next_prng: jax.random.PRNGKeyArray = field(default=None, compare=False)

    # Train Config Settings
    pre_fetch: int = 2

    def __post_init__(self):
        self._next_prng = jax.random.PRNGKey(self.seed)

    def _next_rand(self) -> jax.random.PRNGKeyArray:
        """
        Stateful generate a new PRNG key. The key is generated by splitting `self._next_prng` as such this is
        not pure and so should not be used within a `jit` context.
        :return: a new prng
        """
        rng, self._next_prng = jax.random.split(self._next_prng)
        return rng

    def _make_rngs(self) -> dict[str, jax.random.PRNGKeyArray]:
        """
        Make a dict of rngs to be passed to apply calls
        :return:
        """
        rng = self._next_rand()
        rngl = jax.random.split(rng, num=len(self.rng_keys))
        rngs = {k: rngl[i] for i, k in enumerate(self.rng_keys)}
        return rngs

    def _train_step(self, batch):
        """
        Run a single step
        :param batch: Batch data
        :return:
        """
        r_state = replicate(self.state)
        rngs = replicate(self._make_rngs())
        r_state, metrics = self.train_step(r_state, batch, rngs)
        self.state, metrics = unreplicate((r_state, metrics))
        return metrics

    @partial(jax.pmap, static_broadcasted_argnums=(0,), axis_name="batch")
    def train_step(self, state: TrainState, batch: Batch, rngs=None) -> State_Result:
        """
        Train for a single step.
        TODO:
         - support multiple output / multi loss via dicts akin to keras
        Notes:
            In order to keep this a pure function, we don't update the `self.state` just return a new state
        """
        x = batch[0]
        y = batch[1]

        GLOBAL_BATCH = np.product(list(y.values())[0].shape[:2])

        @partial(jax.value_and_grad, has_aux=True)
        def step(params):
            y_pred = state.apply_fn({'params': params}, x, rngs=rngs)
            p_loss = self.loss_fn(y, y_pred)
            loss = p_loss.sum() / GLOBAL_BATCH
            return loss, y_pred

        (loss, y_pred), grads = step(state.params)
        grads = jax.lax.psum(grads, axis_name="batch")
        state = state.apply_gradients(grads=grads)
        metrics = dict(loss=loss, **self.metrics_fn(y, y_pred))
        metrics = jax.lax.pmean(metrics, axis_name='batch')
        return state, metrics

    @partial(jax.pmap, static_broadcasted_argnums=(0,), axis_name="batch")
    def pred_step(self, state: TrainState, batch:Batch, rngs=None):
        y_pred = state.apply_fn({'params': state.params}, batch[0], rngs=rngs)
        return (*batch, y_pred)
    
    def map_model(self, data: tfd.Dataset):
        """
        Transform a dataset to
        :param data:
        :return:
        """
        data = add_device_batch(data)
        d_iter = data.as_numpy_iterator()
        d_iter = iter(Prefetch(d_iter, buffer_size=self.pre_fetch))
        state = replicate(self.state)
        while True:
            if not (batch := next(d_iter, None)):
                break
            rngs = replicate(self._make_rngs())
            yield self.pred_step(state, batch, rngs)

    @partial(jax.pmap, static_broadcasted_argnums=(0,), axis_name="batch")
    def test_step(self, state: TrainState, batch: Batch, rngs=None) -> State_Result:
        x = batch[0]
        y = batch[1]
        y_pred = state.apply_fn({'params': state.params}, x, rngs=rngs)
        loss = self.loss_fn(batch[1], y_pred).mean()
        metrics = dict(loss=loss, **self.metrics_fn(y, y_pred))
        metrics = jax.lax.pmean(metrics, axis_name='batch')
        return state, metrics

    def _stateful_step_runner(self, data: tfd.Dataset, step_fn: Callable[[TrainState, Batch, Any], State_Result],
                              hist: list,
                              start_cb: CallbackFn, step_start_cb: CallbackFn,
                              end_cb: CallbackFn, step_end_cb: CallbackFn) -> None:
        """
        A standard step call, helpful to reduce code in the main train loops
        :param data: data to iterate over
        :param step_fn: the step function to call, must be
        :param hist:
        :param start_cb:
        :param step_start_cb:
        :param end_cb:
        :param step_end_cb:
        :return:
        """
        start_cb(self)
        d_iter = data.as_numpy_iterator()
        d_iter = iter(Prefetch(d_iter, buffer_size=self.pre_fetch))
        # Replicate state to all devices, use this ref over self.state to reduce / broadcast calls
        r_state = replicate(self.state)
        step = int(self.state.step)
        while True:
            with jax.profiler.StepTraceAnnotation("train", step_num=step):
                if not (batch := next(d_iter, None)):
                    break
                step_start_cb(self)
                rngs = replicate(self._make_rngs())
                r_state, metrics = step_fn(r_state, batch, rngs)
                self.state, metrics = unreplicate((r_state, metrics))   # un-replicate so callbacks and metrics work
                hist.append(metrics)
                step += int(self.state.step)  # eval step keep in sync with GPU
                step_end_cb(self)
        end_cb(self)

    def fit(self, train_data: tfd.Dataset, val_data: tfd.Dataset = None, num_epochs: int = 1):
        self.num_epochs = num_epochs

        train_cardinality = int(train_data.cardinality())
        self.train_steps = train_cardinality if train_cardinality > 0 else None

        self.eval_steps = None
        if val_data:
            eval_cardinality = int(val_data.cardinality())
            self.eval_steps = eval_cardinality if eval_cardinality > 0 else None

        # Check to make sure the data isn't all the same value. It's happened, it's a pain
        first = train_data.as_numpy_iterator().next()
        pass_sanity, input_errors = sanity_check(first)
        if pass_sanity:
            logging.info("Sanity check passed: %s", input_errors)
        else:
            logging.warning("Sanity check Failed: %s", input_errors)

        train_data = add_device_batch(train_data)
        val_data = val_data if not val_data else add_device_batch(val_data)

        con = Console(color_system="windows", force_interactive=FLAGS.interactive, force_terminal=FLAGS.interactive)
        live = Live(self.train_window, console=con, refresh_per_second=FLAGS.refresh_per_second)
        live.start()

        callbacks = CallbackRunner(callbacks=self.callbacks)
        callbacks.on_fit_start(self)

        for e in range(self.num_epochs):
            epoch_start_time = time.monotonic()
            callbacks.on_epoch_start(self)
            self.train_hist["epochs"].append({"train": [], "test": []})

            # Run Train Step
            self._stateful_step_runner(train_data, self.train_step,  self.train_hist["epochs"][-1]["train"],
                                       callbacks.on_train_start, callbacks.on_train_batch_start,
                                       callbacks.on_train_end, callbacks.on_train_batch_end)

            # Update after first epoch sine they should all be the same size
            if self.train_steps is None:
                self.train_steps = len(self.train_hist["epochs"][-1]["train"])

            # Test model - Only run if there is val_data
            if val_data:
                self._stateful_step_runner(val_data, self.test_step, self.train_hist["epochs"][-1]["test"],
                                           callbacks.on_test_start, callbacks.on_test_batch_start,
                                           callbacks.on_test_end, callbacks.on_test_batch_end)

                # Update after first epoch sine they should be the same size
                if self.eval_steps is None:
                    self.eval_steps = len(self.train_hist["epochs"][-1]["test"])

            # End of epoc metrics
            mean_train = mean_reduce_dicts(self.train_hist["epochs"][-1]["train"])
            mean_test = mean_reduce_dicts(self.train_hist["epochs"][-1]["test"])
            mean_test = {f"val_{k}": v for k, v in mean_test.items()}  # Add `val` prefix to test metrics
            mets = dict(**mean_train, **mean_test)
            self.train_hist["train"].append(mets)

            # TODO: Maybe move this to a logging callback
            met = make_metric_string(mets)
            duration = time.monotonic() - epoch_start_time
            duration = seconds_pretty(duration)
            logging.info(f"Epoch: {e} - {duration}  {met}")

            # End of epoch callbacks
            callbacks.on_epoch_end(self)

        callbacks.on_fit_end(self)
        live.stop()  # Close the live window since we aren't in a contex
        return
