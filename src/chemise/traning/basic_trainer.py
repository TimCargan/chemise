from __future__ import annotations

import operator
import time
from dataclasses import dataclass, field
from functools import partial, reduce
from typing import Callable, Any, Tuple, List

import jax
import numpy as np
from absl import logging, flags
from flax.jax_utils import replicate, unreplicate
from flax.training.train_state import TrainState
from jaxtyping import Num, Array
from rich.console import Console
from rich.layout import Layout
from rich.live import Live
from tensorflow import data as tfd  # Only for typing

from chemise.callbacks.abc_callback import Callback, CallbackRunner, StepCallback
from chemise.traning.prefetch import Prefetch
from chemise.utils import mean_reduce_dicts, make_metric_string, seconds_pretty

flags.DEFINE_bool("interactive", default=False, help="Run in interactive mode. e.g print graphs", short_name='i')
flags.DEFINE_float("refresh_per_second", default=0.2, help="Frequency in Hz to redraw in interactive mode")

FLAGS = flags.FLAGS

State_Result = Tuple[TrainState, dict]
Features = dict[str, Num[Array, "..."]]
Batch = Tuple[Features, Features]


def empty_train_hist():
    return {"epochs": [], "train": []}


def make_default_layout() -> Layout:
    layout = Layout(name="root")
    layout.split(
        Layout(name="graph", ratio=1),
        Layout(name="progbar", size=5),
    )
    return layout


def add_device_batch(data: tfd.Dataset) -> tfd.Dataset:
    platform = jax.default_backend()
    d_count = jax.device_count(platform)
    logging.log_first_n(logging.INFO, "Running on %s with %d devices", 1, platform, d_count)
    logging.debug("Adding device batch of size (%d) to dataset: %s", d_count, data.element_spec)
    data = data.batch(d_count, drop_remainder=True).prefetch(2)
    return data


def _sanity_error(data: Features) -> (bool, dict):
    """
    Determine if all values for each key in a dict of Num's are the same
    Note: This has the opposite result to `sanity_check` which returns True if the data is different
    :param data:
    :return: flag - True if are key has all the same values, dict of keys with v True if all values are the same
    """
    feats = {f"{k}": np.all(v == v[0], axis=None) for k, v in data.items()}
    is_good = reduce(operator.or_, feats.values(), False)
    return is_good, feats


def sanity_check(data: Batch):
    """
    Check to see if the input and label data looks correct, i.e not all the same value
    :param data:
    :return: bool - True if all values different, dict - input keys and a bool set to True if value is all the same
    """
    r_inputs, inputs = _sanity_error(data[0])
    inputs = {f"I_{k}": np.reshape(data[0][k], (-1,))[...,0] for k, v in inputs.items() if v}
    r_labels, labels = _sanity_error(data[1])
    labels = {f"O_{k}": np.reshape(data[1][k], (-1,))[..., 0] for k, v in labels.items() if v}
    is_good = np.logical_not(np.logical_or(r_inputs, r_labels))
    return is_good, dict(**inputs, **labels)


@jax.tree_util.Partial
def no_metrics_fn(y, y_hat):
    return {}


@dataclass(unsafe_hash=True)
class BasicTrainer:
    """
    Implement boilerplate helper methods to fit basic models similar to what we get in keras
    This class can manage all the state for the various jax / flax object needed to run basic NN training
    We want to have a few basic methods
     - Fit
     - Predict
     - TODO: Transform - same as predict but just add the predictions the input data
    """

    state: TrainState = field(compare=False)
    loss_fn: Callable[[Num[Array, "..."] | dict[str, Num[Array, "..."]], Num[Array, "..."]], Num[Array, "..."]]
    metrics_fn: Callable[[Num[Array, "..."], Num[Array, "..."]], dict] = no_metrics_fn
    callbacks: [Callback] = field(default_factory=list, compare=False)
    train_hist: dict[str, list[Any]] = field(default_factory=empty_train_hist, compare=False)
    train_window: Layout = field(default_factory=make_default_layout, compare=False)

    rng_keys: List[str] = field(default_factory=list, compare=False)
    seed: int = 0
    _next_prng: jax.random.PRNGKeyArray = field(default=None, compare=False)

    # Train Config Settings
    pre_fetch: int = 2

    def __post_init__(self):
        self._next_prng = jax.random.PRNGKey(self.seed)

    def _next_rand(self) -> jax.random.PRNGKeyArray:
        """
        Stateful generate a new PRNG key. The key is generated by splitting `self._next_prng` as such this is
        not pure and so should not be used within a `jit` context.
        :return: a new prng
        """
        rng, self._next_prng = jax.random.split(self._next_prng)
        return rng

    def _make_rngs(self) -> dict[str, jax.random.PRNGKeyArray]:
        """
        Make a dict of rngs to be passed to apply calls
        TODO: Look into using mixin
        :return:
        """
        rng = self._next_rand()
        rngl = jax.random.split(rng, num=len(self.rng_keys))
        rngs = {k: rngl[i] for i, k in enumerate(self.rng_keys)}
        return rngs

    def step(self, batch: Batch):
        """
        Run a single step without any jax transformations, helpful for debugging
        :param batch:
        :return: [predictions, loss, metrics]
        """
        x, y = batch
        rngs = self._make_rngs()
        y_pred = self.state.apply_fn({'params': self.state.params}, batch[0], rngs=rngs)
        p_loss = self.loss_fn(y, y_pred)
        met = self.metrics_fn(y, y_pred)
        return (y_pred, p_loss, met)

    def _train_step(self, batch):
        """
        Run a single step
        :param batch: Batch data
        :return:
        """
        r_state = replicate(self.state)
        rngs = replicate(self._make_rngs())
        r_state, metrics = self.train_step(r_state, batch, rngs)
        self.state, metrics = unreplicate((r_state, metrics))
        return metrics

    @partial(jax.pmap, static_broadcasted_argnums=(0,), axis_name="batch")
    def train_step(self, state: TrainState, batch: Batch, rngs=None) -> State_Result:
        """
        Train for a single step.
        TODO:
         - support multiple output / multi loss via dicts akin to keras
        Notes:
            In order to keep this a pure function, we don't update the `self.state` just return a new state
        """
        x = batch[0]
        y = batch[1]

        GLOBAL_BATCH = np.product(list(y.values())[0].shape[:2])

        @partial(jax.value_and_grad, has_aux=True)
        def step(params):
            """
            Run a single step with grads calculated for backprop
            :param params: params of model
            :return: [Loss, predictions]
            """
            y_pred = state.apply_fn({'params': params}, x, rngs=rngs)
            p_loss = self.loss_fn(y, y_pred)
            loss = p_loss.sum() / GLOBAL_BATCH
            return loss, y_pred

        (loss, y_pred), grads = step(state.params)
        grads = jax.lax.psum(grads, axis_name="batch")
        state = state.apply_gradients(grads=grads)
        metrics = dict(loss=loss, **self.metrics_fn(y, y_pred))
        metrics = jax.lax.pmean(metrics, axis_name='batch')
        return state, metrics

    @partial(jax.pmap, static_broadcasted_argnums=(0,), axis_name="batch")
    def pred_step(self, state: TrainState, batch: Batch, rngs=None):
        """
        Apply model to a batch of data returning
        :param state: model state object
        :param batch: Batch tuple to predict with
        :param rngs: dict of rngs for use in the model
        :return: tuple of [X, Y, Y_hat]
        """
        y_pred = state.apply_fn({'params': state.params}, batch[0], rngs=rngs)
        return (*batch, y_pred)

    def map_model(self, data: tfd.Dataset):
        """
        Map the model over the dataset
        Transforming it to include predictions
        :param data: dataset to map over
        :return: an iterator that yields [X, Y, Y_hat]
        """
        data = add_device_batch(data)
        d_iter = data.as_numpy_iterator()
        d_iter = iter(Prefetch(d_iter, buffer_size=self.pre_fetch))
        state = replicate(self.state)
        while True:
            if not (batch := next(d_iter, None)):
                break
            rngs = replicate(self._make_rngs())
            yield self.pred_step(state, batch, rngs)

    @partial(jax.pmap, static_broadcasted_argnums=(0,), axis_name="batch")
    def test_step(self, state: TrainState, batch: Batch, rngs=None) -> State_Result:
        """
        Perform a prediction step and calculate metrics for a given batch
        :param state: model state object
        :param batch: Batch tuple to predict with
        :param rngs: dict of rngs for use in the model
        :return: [State, dict metrics]
        """
        x = batch[0]
        y = batch[1]
        y_pred = state.apply_fn({'params': state.params}, x, rngs=rngs)
        loss = self.loss_fn(batch[1], y_pred).mean()
        metrics = dict(loss=loss, **self.metrics_fn(y, y_pred))
        metrics = jax.lax.pmean(metrics, axis_name='batch')
        return state, metrics

    def _stateful_step_runner(self, data: tfd.Dataset, step_fn: Callable[[TrainState, Batch, Any], State_Result],
                              hist: list, callback: StepCallback) -> None:
        """
        A standard step call, helpful to reduce code in the main train loops
        :param data: data to iterate over
        :param step_fn: the step function to call, must be
        :param hist:
        :param callback: StepCallback object
        :return:
        """
        callback.start_cb(self)
        d_iter = data.as_numpy_iterator()
        d_iter = iter(Prefetch(d_iter, buffer_size=self.pre_fetch))
        # Replicate state to all devices, use this ref over self.state to reduce / broadcast calls
        r_state = replicate(self.state)
        step = int(self.state.step)
        while True:
            with jax.profiler.StepTraceAnnotation("train", step_num=step):
                if not (batch := next(d_iter, None)):
                    break
                callback.step_start_cb(self)
                rngs = replicate(self._make_rngs())
                r_state, metrics = step_fn(r_state, batch, rngs)
                self.state, metrics = unreplicate((r_state, metrics))  # un-replicate so callbacks and metrics work
                hist.append(metrics)
                step += int(self.state.step)  # eval step keep in sync with GPU
                callback.step_end_cb(self)
        callback.end_cb(self)

    def fit(self, train_data: tfd.Dataset, val_data: tfd.Dataset = None, num_epochs: int = 1):
        """
        Fit model to a given dataset
        :param train_data: data to fit the model to
        :param val_data: validation data to
        :param num_epochs: number of epochs to appy the data
        :return:
        """
        self.num_epochs = num_epochs

        train_cardinality = int(train_data.cardinality())
        self.train_steps = train_cardinality if train_cardinality > 0 else None

        self.eval_steps = None
        if val_data:
            eval_cardinality = int(val_data.cardinality())
            self.eval_steps = eval_cardinality if eval_cardinality > 0 else None

        # Check to make sure the data isn't all the same value. It's happened, it's a pain
        first = train_data.as_numpy_iterator().next()
        pass_sanity, input_errors = sanity_check(first)
        if pass_sanity:
            logging.info("Sanity check passed: %s", input_errors)
        else:
            logging.warning("Sanity check failed, The following keys all have the same value: %s", input_errors)

        train_data = add_device_batch(train_data)
        val_data = val_data if not val_data else add_device_batch(val_data)

        con = Console(color_system="windows", force_interactive=FLAGS.interactive, force_terminal=FLAGS.interactive)
        live = Live(self.train_window, console=con, refresh_per_second=FLAGS.refresh_per_second)
        live.start()

        callbacks = CallbackRunner(callbacks=self.callbacks)
        callbacks.on_fit_start(self)

        for e in range(self.num_epochs):
            epoch_start_time = time.monotonic()
            callbacks.on_epoch_start(self)
            self.train_hist["epochs"].append({"train": [], "test": []})

            # Run Train Step
            self._stateful_step_runner(train_data, self.train_step, self.train_hist["epochs"][-1]["train"],
                                       callbacks.train_step_callbacks())

            # Update after first epoch sine they should all be the same size
            if self.train_steps is None:
                self.train_steps = len(self.train_hist["epochs"][-1]["train"])

            # Test model - Only run if there is val_data
            if val_data:
                self._stateful_step_runner(val_data, self.test_step, self.train_hist["epochs"][-1]["test"],
                                           callbacks.test_step_callbacks())

                # Update after first epoch sine they should be the same size
                if self.eval_steps is None:
                    self.eval_steps = len(self.train_hist["epochs"][-1]["test"])

            # End of epoc metrics
            mean_train = mean_reduce_dicts(self.train_hist["epochs"][-1]["train"])
            mean_test = mean_reduce_dicts(self.train_hist["epochs"][-1]["test"])
            mean_test = {f"val_{k}": v for k, v in mean_test.items()}  # Add `val` prefix to test metrics
            mets = dict(**mean_train, **mean_test)
            self.train_hist["train"].append(mets)

            # TODO: Maybe move this to a logging callback
            met = make_metric_string(mets)
            duration = time.monotonic() - epoch_start_time
            duration = seconds_pretty(duration)
            logging.info(f"Epoch: {e} - {duration}  {met}")

            # End of epoch callbacks
            callbacks.on_epoch_end(self)

        callbacks.on_fit_end(self)
        live.stop()  # Close the live window since we aren't in a contex
        return
